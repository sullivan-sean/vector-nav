---
title: "Vector-based Navigation Using (Semi) Grid-Like Representations"
date: "April 30, 2019"
author: "Sean Sullivan"
bibliography: sources.bib
header-includes: |
  \usepackage{setspace}
  \usepackage{indentfirst}
  \usepackage{caption}
  \usepackage{graphicx}
  \usepackage{gensymb}
  \usepackage{amsmath}
  \usepackage{amssymb}
  \usepackage{bm}
papersize: a4paper
documentclass: article
fontsize: 12pt
geometry: "margin=1in"
---

\setlength{\parindent}{0.3in}
\setlength{\parskip}{0pt}
\onehalfspacing

## Introduction

Representations similar to grid cells can emerge in supervised
training of rat-like motion models used for navigation within an enclosure.
Besides being an efficient coding mechanism for the information in place and
head direction cells, it is unclear why this phenomenon arises and whether the
emergence of these representations are robust to alterations of the model. In
particular, the robustness of grid-like representations in vector based
navigation tasks has not been shown for variations in initialization,
regularization, varied motion models. In this work, I replicate previous models
that demonstrate the emergence of grid-like neurons but train them under
various experimental alterations to observe and better understand the
boundaries of the grid cellsâ€™ emergence. Recent work by has shown that
grid-like representations emerge naturally when training an agent to navigate
small enclosures under a random motion model [@Banino2018; @Cueva2018].
To better understand the results of this paper, when they hold and to what
degree, further experiments on a similar model must be run. I hypothesize that,
consistent with the results of previous authors, these phenomena will emerge,
under variations to the models.

## Methods

The model used is based off of a rat-like motion model, inspired by
@Raudies2012.  This model simulates a rat as an agent in a square or circular
environment, or scene. Within the scene, the rat moves forward each time step
with a random velocity sampled from a Rayleigh distribution, for which I used a
scale parameter of 0.13 m/s. The rat also rotates randomly with an angular
velocity randomly sampled from a normal distribution with a standard deviation
of $330^\circ$ per second and a mean of $0^\circ$. When the rat is within 0.03
meters of a wall, it turns an additional $90^\circ$ per second away from its
current facing direction. These distributions and measurements are based on
both those from Raudies, et al. as well as Banino et al., who both used similar
models in their research based on empirical phenomena of rat movement
[@Raudies2012]. To generate the trajectories, this simulation process was run
for 750 time steps with a delta of 0.02s each time step to generate 15s worth
of movement. At each step, the velocities were randomly generated as specified
and then used to calculate the next position and head direction. The initial
position was selected uniformly at random from within the scene and the initial
head direction was sampled uniformly at random from around the unit circle.
From the trajectories of 750 points, 100 linearly spaced points are taken as a
point in our data set. Each of these points consists of 5 components, the
velocities for each time step (angular and forward), the position at each time
step, the head direction at each time step, the initial position, and the
initial head direction. An example of trajectories generated by this algorithm
are shown for a square and circular cage below. The square has side length of
2.2 meters and the circle has diameter of 2.2 meters.
\begin{figure}[!h]
\centering
\captionsetup{justification=centering}
\includegraphics[width=.4\linewidth]{../img/square_scene}
\includegraphics[width=.4\linewidth]{../img/circle_scene}
\caption{
Square and circular scenes depicting generated trajectories. Trajectories cover
the space uniformly and at random, with some bias towards staying near the
walls. Each colored line represents a different trajectory, with 8 trajectories
shown in each scene.
}
\end{figure}
These trajectories are then used to train a model similar to that from Banino
et al. We use a supervised model that takes as input a sequence of velocities,
and outputs predicted place cell and head direction cell activations. We use
an ensemble of 256 place cells that we model as a two dimensional mixture of
Guassians:
$$\bm{c}(x) =\text{softmax}\left(-\frac{||x - \bm{\mu}||_2^2}{2\sigma^2}\right)$$
where $\bm{c}:\mathbb{R}^2\to[0,1]^{256}$ maps a position to a probability
distribution over the place cells with $\bm{\mu}\in\mathbb{R}^{256\times 2}$
the place cell centers which are fixed throughout an experiment after being
sampled uniformly at random over the scene. The standard deviation of the
Guassians, $\sigma$ is also fixed as the scale of the place cells for each
experiment at a value of 0.01. Similarly, we model the head-direction cells
with an ensemble of $12$ cells, again with a probability distribution over the
ensemble with a mixture of Von Mises distributions:
$$\bm{h}(\phi) = \text{softmax} \left(\kappa \cos(\phi - \bm{\mu})\right)$$
with $\bm{h}:\mathbb{R}^2\to[0,1]^{12}$ takes a facing angle gives a
distribution over the cells with centers at $\bm{\mu}\in[-\pi,\pi]^{12}$. The
centers are again sampled uniformly at random for each experiment. The
concentration parameter $\kappa$ is fixed to a value of $20$.

Given a trajectory with head directions $\phi$ and positions $x$ at each time
step, $\bm{c}(x)$ and $\bm{h}(\phi)$ give us output sequence of place cell and
head direction cell activations. These will be used as the supervised output
for our model. The input to the model will egocentric velocities at each time
step of a trajectory. For a single trajectory this is 100 time steps with 3
components at each step: $[v_t, \cos(\dot{\phi_t}), \sin(\dot{\phi_t})]$, the
forward velocity and the sine and cosine of the angular velocity.

Given the velocity inputs, the model will predict the activations of the place
and head direction cells, effectively performing path integration that can
be recovered from an argmax over the place cell distribution at each time step.

The network architecture is as follows: the $3\times 100$ vector is fed into an
LSTM (Long Short Term Memory) layer with hidden dimension 128
[@hochreiter1997lstm]. The initial cell and hidden state of the LSTM are
computed as linear combinations of the initial place cell and head direction
cell activations, $\bm{c}(x_0)$ and $\bm{h}(\phi_0)$. Thus we have the inital
states of the LSTM are:
\begin{align*}
  cs &= W_1 \bm{c}(x_0) + W_2 \bm{h}(\phi_0) + b_1 \\
  hs &= W_3 \bm{c}(x_0) + W_4 \bm{h}(\phi_0) + b_2
\end{align*}
We learn the weight matrices and biases through backprop during training.
After passing the sequence of velocities through the recurrent LSTM layer we
have a vector of size $128$ at each time step. This sequence is passed through
a linear layer with output size 256 to obtain a vector $g\in \mathbb{R}^{256}$
for each time step. This is the layer in which we see grid like representations
emerge. Dropout is applied to the output of this linear layer with probability 
0.5 [@Srivastava14]. Finally, this output is passed into two further linear
layers, one to the head direction cells with output size $12$ and the other to
the place cells with output size $256$. Each of these outputs is normalized by
applying a softmax function to obtain a predicted distribution $\bm{y}$ over
place cells and $\bm{z}$ over head direction cells at each time step. We
minimize the cross entropy loss between these predictions and the ground truth
values. Weight decay of $1\times 10^{-5}$ was applied to the final 3 linear
layers and gradient clipping to the range $[-1\times 10^{-5},1\times 10^{-5}]$.
This clipping prevents gradients from growing too large during backpropagation
through the recurrent LSTM model. A visualization of the model architecture can
be seen in figure 2.

\begin{figure}[!h]
\centering
\captionsetup{justification=centering}
\includegraphics[width=0.4\linewidth]{../img/model}
\caption{
As in Banino et al., linear layers project initial head cell and place cell
distributions to the initial state of the LSTM, which is fed egocentric
velocities as input. The output of the recurrent layer is fed through three
linear layers to make prediction of head and place cell activations through an
intermediary grid layer.
}
\end{figure}

This supervised model is inspired by the development of the entorhinal cortex
and hippocampus. There is evidence that in rodents, grid cells emerge after
the appearance of place cells and head direction cells [@Wills2010]. Furthermore,
both of these cell ensembles sit very close to and could impact the development
of grid cells [@Barry2014]. This is demonstrated in our model as the place and head
direction cell activation are used directly in the loss function to optimize
the weights in the linear layer that gives rise to the vector $g$. As it is in
this layer that we expect to see emergent grid cells, the relationship roughly
parallels the biological development of these cells. Finally, in developed
rodents, grid cells have been observed to influence the activity of place cells
via connections to the hippocampus [@Zhang2013]. This model, then, has a
reasonable biological basis related to the mechanisms known to help animals
path integrate.

## Experiments and Results

The model implemented by Banino et al. produces grid cells given the model
architecture described when trained on the trajectories from their own rat
model. These trajectories, however, have different statistical properties than
those found in Raudies et al. To demonstrate this difference, I measured, over
5000 trajectories, the distribution of position over all time steps in the
trajectory. While the model proposed by Raudies et al. produces a somewhat
uniform distribution over the space, the trajectories from Banino et al. tend
to hug the walls and especially the corners. Example distributions of the two
are shown in figure 3. Note that, while the model from Banino et al. has a much
higher density at the walls, when this is excluded, the distribution is mostly
uniformly random, much like Raudies et al. While this behavior is to be
expected at the edge of an enclosure to some degree, I wanted to investigate if
the emergence of grid cells was dependent on this distribution. 

\begin{figure}[!h]
\centering
\captionsetup{justification=centering}
\includegraphics[width=.2\linewidth]{../img/uniform}
\includegraphics[width=.2\linewidth]{../img/edges}
\includegraphics[width=.2\linewidth]{../img/uniform_crop}
\includegraphics[width=.2\linewidth]{../img/edges_crop}
\caption{
Distribution of position across 5000 trajectories. Red is most frequent while
blue is least frequent. From left to right, distribution generated by Raudies
et al., distribution generated by Banino et al. cropped distribution by Raudies
et al., and cropped distribution by Banino et al. Note that the distributions
are much more similar (both are near uniformly random) when the walls are
cropped out. With walls included, the corners in particular of the second
distribution are dark red, indicating on average the rat spends many more time
steps in the corner.
}
\end{figure}

To test the model's robustness to the distribution of the trajectories, I first
trained the model on Deepmind's trajectories. This replicated their results
and after 40 epochs of 1000 trajectories each, grid-like cells were clearly
present in layer $g$.

\begin{figure}[!h]
\centering
\captionsetup{justification=centering}
\includegraphics[width=.6\linewidth]{../img/plot_40}
\includegraphics[width=.6\linewidth]{../img/grid_40}
\caption{
Spatial activation map of 5 grid cells units after 40 epochs trained on the
Deepmind trajectories. This upper plots represent where in the grid each cell
was active, red being most active and blue being least active. The bottom plots
show the spatial autocorrelation of the maps.
}
\end{figure}

The spatial autocorrelation of the ratemap is a good determinant of how grid
like a cell is and can pick up on grid-like structure that is not apparent
in the ratemap [@Hafting2005]. Here we see that many of the cells are grid-like
after only 40 epochs (in the original paper this model was trained for 1000
epochs). About 30 of 256 of the units in $g$ appear somewhat grid like after
this many epochs. These grid units did not appear, however, in this quantity
at earlier epochs. At epoch 24, for example, there were no grid cells as can
be seen in figure 5.

\begin{figure}[!h]
\centering
\captionsetup{justification=centering}
\includegraphics[width=.6\linewidth]{../img/plot_24}
\includegraphics[width=.6\linewidth]{../img/grid_24}
\caption{
Spatial activation map of 5 grid cells units after 24 epochs trained on the
Deepmind trajectories. None of the 256 units showed any grid patterns in either
the map or the autocorrelograms after this epoch.
}
\end{figure}

In training on the more uniform trajectories, generated from my own model
inspired by Raudies et al., the results are starkly different when we have
trained for only 24 epochs. Here we see that 10 of the 256 units already exibit
grid-like features. This result was replicable across $5$ trials of these
trajectories, with $8-13$ grid like units arising around 24-28 epochs in. This
suggests there could be some dependence on the statistics of the trajectories
in the speed at which grid cells emerge. Another interesting finding, however,
is that after these first grid cells emerged, the model did not improve much
beyond this point. The loss stayed constant and no more grid cells emerged.
In contrast, the Deepmind trajectories produced many more grid cells as reported
intkgg

\begin{figure}[!h]
\centering
\captionsetup{justification=centering}
\includegraphics[width=.6\linewidth]{../img/plot_24_mine}
\includegraphics[width=.6\linewidth]{../img/grid_24_mine}
\caption{
Spatial activation map of 5 grid cells units after 24 epochs trained on the
generated trajectories. Despite the promising look of these results so early,
the cells do not improve beyond this point.
}
\end{figure}

A second expe
The cross-entropy loss graph for the two models is shown below

# Discussion and Conclusions



# Bibliography

\noindent
\vspace{-2em}
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}
\setlength{\parskip}{8pt}
